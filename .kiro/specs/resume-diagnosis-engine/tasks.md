# Implementation Plan: Resume Diagnosis Engine

## Overview

This implementation plan converts the Resume Diagnosis Engine design into discrete coding tasks that build incrementally toward a complete AI-powered resume analysis system. The approach prioritizes core functionality first, with comprehensive testing and security features integrated throughout.

## Tasks

- [x] 1. Set up project infrastructure and database schema
  - Create TypeScript project with PostgreSQL and Redis connections
  - Set up database schema for job titles, role templates, and user sessions
  - Configure Groq API integration and environment variables
  - Set up basic project structure with folders for components, services, and tests
  - _Requirements: 9.1, 9.2, 10.1_

- [x] 1.1 Write property test for database connection
  - **Property 11: Data Protection Round-trip**
  - **Validates: Requirements 9.1, 9.2**

- [x] 2. Implement file upload and parsing system
  - [x] 2.1 Create file upload endpoint with validation
    - Implement file type validation (PDF, DOC, DOCX)
    - Add file size limits and security checks
    - Set up temporary encrypted file storage
    - _Requirements: 1.1, 1.2, 1.3, 10.1, 10.2_

  - [x] 2.2 Write property tests for file processing
    - **Property 1: File Processing Consistency**
    - **Property 2: Invalid File Rejection**
    - **Validates: Requirements 1.1, 1.2, 1.3, 1.4**

  - [x] 2.3 Implement resume text extraction
    - Build PDF text extraction using pdf-parse library
    - Build DOC/DOCX extraction using mammoth library
    - Add extraction confidence scoring
    - Implement structured section parsing (experience, skills, education)
    - _Requirements: 1.1, 1.2, 1.4, 1.5_

  - [x] 2.4 Write unit tests for text extraction
    - Test specific PDF and DOC parsing examples
    - Test extraction confidence calculation
    - _Requirements: 1.1, 1.2_

- [ ] 3. Build job title normalization system
  - [ ] 3.1 Create job title database schema and seed data
    - Design PostgreSQL tables for canonical titles and variations
    - Populate database with common job title mappings
    - Add industry-specific role templates
    - _Requirements: 2.1, 2.3_

  - [ ] 3.2 Implement job title normalizer service
    - Build title lookup and normalization logic
    - Add generic title detection (Manager, Developer, etc.)
    - Implement Groq-powered title suggestion for unknown titles
    - Add specialization prompting for generic titles
    - _Requirements: 2.2, 2.3, 2.4_

  - [ ] 3.3 Write property tests for job title normalization
    - **Property 3: Job Title Normalization**
    - **Property 4: Generic Title Detection**
    - **Validates: Requirements 2.2, 2.3, 2.4**

- [ ] 4. Checkpoint - Ensure file processing and job normalization work
  - Ensure all tests pass, ask the user if questions arise.

- [ ] 5. Implement core diagnosis engine
  - [ ] 5.1 Build deterministic analysis components
    - Implement keyword gap analysis (missing critical terms)
    - Build skills mismatch detection logic
    - Create ATS compatibility checker (formatting, structure)
    - Add evidence location tracking with precise references
    - _Requirements: 3.1, 3.2, 8.3, 8.4_

  - [ ] 5.2 Integrate Groq LLM for model-assisted analysis
    - Set up Groq API client with structured prompts
    - Implement experience level alignment assessment
    - Build achievement quantification analysis
    - Add anti-hallucination validation (evidence must exist in source text)
    - _Requirements: 3.1, 3.2, 3.4_

  - [ ] 5.3 Write property tests for analysis completeness
    - **Property 5: Analysis Completeness**
    - **Property 6: Evidence Requirement**
    - **Validates: Requirements 3.1, 3.2, 3.3, 3.4**

- [ ] 6. Build scoring and recommendation system
  - [ ] 6.1 Implement scoring calculator
    - Build severity scoring (1-10) based on interview impact
    - Implement impact scoring (1-10) based on opportunity breadth
    - Create confidence scoring (0-100) based on data quality
    - Add scoring rubric implementation with thresholds
    - _Requirements: 6.1, 8.1, 8.2, 8.5_

  - [ ] 6.2 Create recommendation generator
    - Build actionable recommendation creation
    - Implement priority ranking by expected impact
    - Add output constraints (max 5 root causes, max 3 recommendations)
    - Ensure recommendations reference specific evidence
    - _Requirements: 4.4, 5.1, 5.2, 5.5_

  - [ ] 6.3 Write property tests for scoring system
    - **Property 7: Score Range Validation**
    - **Property 8: Output Constraints**
    - **Property 9: Recommendation Completeness**
    - **Validates: Requirements 4.4, 5.1, 5.2, 5.5, 6.1, 8.1, 8.2**

- [ ] 7. Implement data security and privacy features
  - [ ] 7.1 Build PII anonymization pipeline
    - Implement name, address, phone, email redaction
    - Create consistent token replacement for analysis
    - Preserve skills and experience data for accuracy
    - Add anonymization validation
    - _Requirements: 9.4_

  - [ ] 7.2 Add data lifecycle management
    - Implement 24-hour TTL for all user data
    - Create automated cleanup jobs with database triggers
    - Add secure deletion with confirmation
    - Build audit logging for all data operations
    - _Requirements: 9.2, 9.3, 9.5_

  - [ ] 7.3 Write property tests for data protection
    - **Property 11: Data Protection Round-trip**
    - **Validates: Requirements 9.1, 9.2, 9.3, 9.4, 9.5**

- [ ] 8. Build API endpoints and response formatting
  - [ ] 8.1 Create main analysis API endpoint
    - Build POST /analyze endpoint accepting resume + job title
    - Implement request validation and error handling
    - Add progress tracking for long-running analysis
    - Create structured JSON response format
    - _Requirements: 3.5, 7.1, 7.2, 7.3_

  - [ ] 8.2 Add performance and resource management
    - Implement request timeouts (30s parsing, 60s analysis, 120s total)
    - Add session limits (one resume per session)
    - Create resource exhaustion prevention
    - Add Redis caching for repeated analyses
    - _Requirements: 1.5, 3.5, 10.3, 10.4, 10.5_

  - [ ] 8.3 Write property tests for system performance
    - **Property 12: System Performance Bounds**
    - **Property 13: Resource Limit Enforcement**
    - **Validates: Requirements 1.5, 3.5, 10.1, 10.2, 10.3, 10.4, 10.5**

- [ ] 9. Implement confidence correlation and quality assurance
  - [ ] 9.1 Build confidence correlation system
    - Implement data quality assessment
    - Create confidence score correlation with data completeness
    - Add uncertainty warnings for low confidence (<60)
    - Build confidence explanation generation
    - _Requirements: 6.2, 6.3, 6.4, 6.5_

  - [ ] 9.2 Write property test for confidence correlation
    - **Property 10: Confidence Correlation**
    - **Validates: Requirements 6.3, 6.4**

- [ ] 10. Integration and end-to-end wiring
  - [ ] 10.1 Wire all components together
    - Connect file upload → parsing → normalization → analysis → scoring → response
    - Implement error handling and graceful degradation
    - Add comprehensive logging and monitoring
    - Test complete user journey from upload to results
    - _Requirements: All requirements integration_

  - [ ] 10.2 Write integration tests
    - Test complete analysis flow with sample resumes
    - Test error scenarios and edge cases
    - Validate security and privacy compliance
    - _Requirements: All requirements validation_

- [ ] 11. Final checkpoint - Comprehensive system validation
  - Ensure all tests pass, ask the user if questions arise.
  - Validate all 13 correctness properties are implemented and tested
  - Confirm security, performance, and quality requirements are met

## Notes

- All tasks are required for comprehensive development from start to finish
- Each task references specific requirements for traceability
- Property tests validate universal correctness properties using fast-check library
- Unit tests validate specific examples and edge cases
- Groq integration provides AI-powered analysis while maintaining evidence traceability
- PostgreSQL provides persistent storage with encryption and audit capabilities
- Redis caching optimizes performance for repeated analyses
